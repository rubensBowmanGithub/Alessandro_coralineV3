define({"topics" : [{"title":"Solutions Overview ","href":"datacollector\/UserGuide\/Solutions\/Overview.html#concept_aw1_p1q_plb","attributes": {"data-id":"concept_aw1_p1q_plb",},"menu": {"hasChildren":false,},"tocID":"concept_aw1_p1q_plb-d46e177808","topics":[]},{"title":"Converting Data to the Parquet Data Format","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to convert Avro files to the columnar format,         Parquet.</p>\n         ","href":"datacollector\/UserGuide\/Solutions\/Parquet.html#concept_jkm_rnz_kx","attributes": {"data-id":"concept_jkm_rnz_kx",},"menu": {"hasChildren":false,},"tocID":"concept_jkm_rnz_kx-d46e177830","topics":[]},{"title":"Automating Impala Metadata Updates for Drift Synchronization for Hive","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to configure a <span class=\"ph\">Drift Synchronization Solution for Hive</span> pipeline to automatically refresh the Impala metadata cache each time changes occur in         the Hive metastore.\n            </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/Impala.html#concept_szz_xwm_lx","attributes": {"data-id":"concept_szz_xwm_lx",},"menu": {"hasChildren":false,},"tocID":"concept_szz_xwm_lx-d46e177855","topics":[]},{"title":"Managing Output Files","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to design a pipeline that writes output files to a         destination, moves the files to a different\n               location, and then changes the permissions for         the files.\n            </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/FileManagement.html#concept_d1q_xl4_lx","attributes": {"data-id":"concept_d1q_xl4_lx",},"menu": {"hasChildren":false,},"tocID":"concept_d1q_xl4_lx-d46e177883","topics":[]},{"title":"Stopping a Pipeline After Processing All Available Data","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to design a pipeline that stops automatically after it         finishes processing all available\n               data.\n            </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/StopPipeline.html#concept_kff_ykv_lz","attributes": {"data-id":"concept_kff_ykv_lz",},"menu": {"hasChildren":false,},"tocID":"concept_kff_ykv_lz-d46e177908","topics":[]},{"title":"Offloading Data from Relational Sources to Hadoop","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to offload data from relational database tables to         Hadoop.</p>\n         ","href":"datacollector\/UserGuide\/Solutions\/SqoopReplacement.html#concept_vrh_jrs_bbb","attributes": {"data-id":"concept_vrh_jrs_bbb",},"menu": {"hasChildren":false,},"tocID":"concept_vrh_jrs_bbb-d46e177933","topics":[]},{"title":"Sending Email During Pipeline Processing","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to design a pipeline to send email notifications at         different moments during pipeline\n               processing.\n            </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/SendEmail.html#concept_t2t_lp5_xz","attributes": {"data-id":"concept_t2t_lp5_xz",},"menu": {"hasChildren":false,},"tocID":"concept_t2t_lp5_xz-d46e177958","topics":[]},{"title":"Preserving an Audit Trail of Events","shortdesc":"\n            <p class=\"shortdesc\">This solution describes how to design a pipeline that preserves an audit trail of         pipeline and stage events that occur.</p>\n         ","href":"datacollector\/UserGuide\/Solutions\/EventStorage.html#concept_ocb_nnl_px","attributes": {"data-id":"concept_ocb_nnl_px",},"menu": {"hasChildren":false,},"tocID":"concept_ocb_nnl_px-d46e177983","topics":[]},{"title":"Loading Data into Databricks Delta Lake","shortdesc":"\n            <p class=\"shortdesc\">You can use several solutions to load data into a Delta Lake table on Databricks. </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/DeltaLake.html#concept_a5b_wvk_ckb","attributes": {"data-id":"concept_a5b_wvk_ckb",},"menu": {"hasChildren":true,},"tocID":"concept_a5b_wvk_ckb-d46e178008","next":"concept_a5b_wvk_ckb-d46e178008",},{"title":"Drift Synchronization Solution for Hive","shortdesc":"\n            <p class=\"shortdesc\">The <span class=\"ph\">Drift Synchronization Solution for Hive</span> detects drift in incoming data and updates corresponding Hive tables. \n            </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/HiveDrift-Overview.html#concept_phk_bdf_2w","attributes": {"data-id":"concept_phk_bdf_2w",},"menu": {"hasChildren":true,},"tocID":"concept_phk_bdf_2w-d46e178844","next":"concept_phk_bdf_2w-d46e178844",},{"title":"<span class=\"ph\">Drift Synchronization Solution for PostgreSQL</span>","shortdesc":"\n            <p class=\"shortdesc\">The <span class=\"ph\">Drift Synchronization Solution for PostgreSQL</span> detects drift in incoming data and automatically creates or alters corresponding         PostgreSQL tables as needed before\n               the data is written.\n            </p>\n         ","href":"datacollector\/UserGuide\/Solutions\/JDBC_DriftSyncSolution.html#concept_ljq_knr_4cb","attributes": {"data-id":"concept_ljq_knr_4cb",},"menu": {"hasChildren":true,},"tocID":"concept_ljq_knr_4cb-d46e181239","next":"concept_ljq_knr_4cb-d46e181239",}]});